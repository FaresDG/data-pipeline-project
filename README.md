# Data Pipeline Project

**Un pipeline complet** pour ingÃ©rer, transformer et visualiser les donnÃ©es Open Data de StatsBomb.

---

## ğŸš€ Overview

Ce projet met en place une architecture de **Data Engineering** end-to-end :

1. **Ingestion** des fichiers JSON de StatsBomb (compÃ©titions, matches, events, lineups) depuis MinIO.
2. **Stockage raw** dans une base **PostgreSQL** (schÃ©ma `pipeline_db.public`).
3. **Orchestration** avec **Apache Airflow** (DAGs pour ingestion et chargement dans PostgreSQL).
4. **Transformation & modÃ©lisation** via **dbt** (layers staging, intermediate, fact/dimension).
5. **Exploration & reporting** avec **Apache Superset**.

> Cette stack 100% conteneurisÃ©e utilise Docker & Docker Compose pour faciliter le dÃ©ploiement local.

---

## ğŸ“¦ Architecture & Services

```text
+---------------+        +---------+        +-------------+
| StatsBomb API |  --->  |   MinIO |  --->  | Airflow DAGs |
+---------------+        +---------+        +-------------+
                                              |           
                                              v           
                                         +-----------+    
                                         | PostgreSQL|    
                                         +-----------+    
                                              |           
                                              v           
                                           +-----+        
                                           | dbt |        
                                           +-----+        
                                              |           
                                              v           
                                        +-------------+   
                                        |  Superset   |   
                                        +-------------+   
```

* **MinIO** : bucket S3 local pour stocker les JSON StatsBomb.
* **PostgreSQL** : persistance des donnÃ©es raw + mÃ©tadonnÃ©es Airflow/Superset.
* **Airflow** : exÃ©cution planifiÃ©e (DAGs `statsbomb_full_ingestion` et `statsbomb_load_postgres`).
* **dbt** : modÃ©lisation SQL, documentation et tests.
* **Superset** : UI pour dashboards & exploration.

---

## ğŸ“‚ Structure du dÃ©pÃ´t

```text
â”œâ”€â”€ dags/                    # Airflow DAGs
â”‚   â”œâ”€â”€ statsbomb_ingestion.py   # Ingestion depuis MinIO vers raw tables
â”‚   â”œâ”€â”€ statsbomb_load_to_postgres.py  # Upsert raw -> tables de schÃ©ma public_intermediate
â”‚   â””â”€â”€ dbt_pipeline.py         # (optionnel) wrapper DockerOperator
â”œâ”€â”€ statsbomb_dbt/           # Projet dbt
â”‚   â”œâ”€â”€ dbt_project.yml      
â”‚   â”œâ”€â”€ models/              # staging/, intermediate/, fact_dim/
â”‚   â”œâ”€â”€ analyses/, macros/, tests/, snapshots/  
â”‚   â””â”€â”€ profiles.yml (via volume dbt_profiles)
â”œâ”€â”€ dbt_profiles/            # profiles.yml (connexion Postgres)
â”œâ”€â”€ plugins/                 # Ã©ventuels plugins Airflow
â”œâ”€â”€ logs/                    # logs Airflow (ignorÃ©s par Git)
â”œâ”€â”€ .env                     # variables dâ€™environnement (ignorÃ© par Git)
â”œâ”€â”€ .gitignore               # rÃ¨gles Git ignore
â”œâ”€â”€ create_tables.sql        # DDL pour raw tables (compÃ©titions, matches, events, lineups)
â”œâ”€â”€ docker-compose.yml       # DÃ©finition des 5 services Docker
â”œâ”€â”€ Dockerfile-airflow       # Image custom Airflow (providers S3, Postgres, Docker)
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python pour scripts dâ€™ingestion
â”œâ”€â”€ superset_config.py       # Config Superset (connexion metadata DB)
â””â”€â”€ README.md                # â† Vous Ãªtes ici !
```

---

## âš™ï¸ PrÃ©requis

* **Docker** (â‰¥â€¯20.10)
* **Docker Compose** (â‰¥â€¯1.27)
* (Optionnel) Proxy HTTP/HTTPS configurÃ© via `HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`

---

## ğŸ”§ Configuration

1. Dupliquez le fichier `.env.template` en `.env` Ã  la racine.
2. Remplissez les variables :

   ```dotenv
   # MinIO (S3)
   MINIO_ROOT_USER=minioadmin
   MINIO_ROOT_PASSWORD=minio123

   # PostgreSQL
   POSTGRES_USER=postgres
   POSTGRES_PASSWORD=postgres
   POSTGRES_DB=pipeline_db

   # Airflow
   FERNET_KEY=<une clÃ© Fernet gÃ©nÃ©rÃ©e>
   AIRFLOW_SECRET_KEY=<mot de passe webserver>

   # (optionnel) proxy
   HTTP_PROXY=
   HTTPS_PROXY=
   NO_PROXY=
   ```

> **Ne commitez jamais** votre vrai `.env` sur GitHub.

---

## â–¶ï¸ Lancer la stack

```bash
   git clone https://github.com/FaresDG/data-pipeline-project.git
   cd data-pipeline-project

   # Construire et dÃ©marrer tous les services
   docker-compose up -d --build

   # (1) Initialiser la base Airflow / Superset
   # Airflow va crÃ©er un utilisateur `admin` par dÃ©faut

   # (2) VÃ©rifiez que tous les conteneurs sont "Up"
   docker-compose ps
```

**Ports exposÃ©s** :

* 9000 â†’ MinIO Console
* 5432 â†’ PostgreSQL
* 8080 â†’ Airflow UI
* 8085 â†’ dbt Docs
* 8088 â†’ Superset UI

---

## ğŸ“ Utilisation

### 1. Airflow

* **URL**: `http://localhost:8080`
* **DAGs disponibles**:

  * `statsbomb_full_ingestion` : ingÃ¨re JSON â†’ raw PostgreSQL
  * `statsbomb_load_postgres` : charge raw â†’ tables finalisÃ©es

### 2. dbt

```bash
   # dans un shell dbt (container dbt)
   docker-compose exec dbt bash
   dbt debug            # vÃ©rifie la connexion
   dbt run              # exÃ©cute tous les modÃ¨les
   dbt test             # lance les tests dÃ©finis
   dbt docs generate   
   dbt docs serve       # UI sur http://localhost:8085
```

### 3. Superset

* **URL**: `http://localhost:8088`
* **Connexion**:

  * Metadata DB = PostgreSQL `pipeline_db` (via `superset_config.py`)
* **CrÃ©er un dataset** pointant sur les tables modÃ©lisÃ©es par dbt (`public_intermediate` schema).

---

## ğŸ§¹ ArrÃªter & Nettoyer

```bash
   docker-compose down --volumes --remove-orphans
```

---

## ğŸ“– Documentation & RÃ©fÃ©rences

* **StatsBomb Open Data** â€“ [https://github.com/statsbomb/open-data](https://github.com/statsbomb/open-data)
* **Apache Airflow** â€“ [https://airflow.apache.org](https://airflow.apache.org)
* **dbt** â€“ [https://docs.getdbt.com](https://docs.getdbt.com)
* **Apache Superset** â€“ [https://superset.apache.org](https://superset.apache.org)

---

> *Bon pipeline !* ğŸš€
> **Fares D.** â€“ Mai 2025
